{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of speaker_id -> label map\n",
    "\n",
    "We recover the labels from the \"SPEAKERS.TXT\" file and create a map from speaker ids to their gender, encoded 0 for Male and 1 for Female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We first read the file with the labels line by line\n",
    "label_path = os.path.join('LibriSpeech', 'SPEAKERS.TXT')\n",
    "with open(label_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "## We create a list of id-gender\n",
    "lines = lines[12:]\n",
    "\n",
    "## selects the subsets that belong to the dev-clean dataset\n",
    "subsets = (np.array([l[11:20] for l in lines]) == 'dev-clean')\n",
    "lines = np.array([l[:9].split('|') for l in lines])[subsets]\n",
    "\n",
    "### We map male to 0 and female to 1\n",
    "labels_map = {\n",
    "    'M' : 0,\n",
    "    'F' : 1\n",
    "}\n",
    "lines = [[int(l[0].strip()), labels_map[l[1].strip()]] for l in lines]\n",
    "\n",
    "## We change it to a map for convenience \n",
    "id_to_labels_map = {}\n",
    "\n",
    "for l in lines:\n",
    "    id_to_labels_map[l[0]] = l[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of the clean dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I compute the envelope of the signal and filter out the parts that have a too low amplitude because they don't add any relevant information for the classification. We create a new binary file with all the clean audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = 'data_clean.txt'\n",
    "\n",
    "## We also downsample as we only need low frequencies for speech recognition\n",
    "sampling_rate = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_envelope(x, sr, threshold = 0.0005):\n",
    "    \"\"\"\n",
    "    This function filters out part of the signal that has a too low amplitude according to its envelope\n",
    "    \"\"\"\n",
    "    y = pd.Series(np.abs(x))\n",
    "    x_mean = y.rolling(window = int(sr/10), min_periods = 1, center = True).mean()\n",
    "    return x[x > x_mean]\n",
    "\n",
    "def create_clean_file(file_name):\n",
    "    \"\"\"\n",
    "    fetches all the audio, converts them into mfccs,\n",
    "    combines them with according labels \n",
    "    and stores them in the specified file name\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    main_path = os.path.join('LibriSpeech', 'dev-clean')\n",
    "    for speaker_id in id_to_labels_map.keys():\n",
    "        speaker_path = os.path.join(main_path, str(speaker_id))\n",
    "        speaker_label = id_to_labels_map[speaker_id]\n",
    "        for folder in os.listdir(speaker_path):\n",
    "            speech_path = os.path.join(speaker_path, folder)\n",
    "            for e in os.listdir(speech_path):\n",
    "                if e.endswith('.flac'):\n",
    "                    x, sr = librosa.load(os.path.join(speech_path, e), sr = sampling_rate)\n",
    "                    x_filtered = filter_envelope(x, sr)\n",
    "                    data.append([x_filtered, speaker_label])\n",
    "                    \n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "create_clean_file(FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCCs computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we fetch the clean data and cut each data element in chunk according to the minimal frame length so that we have a consistent input length for our future models.\n",
    "We then compute the MFCCs for each of the chunks and store them in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lengths(data, sr):\n",
    "    \"\"\"\n",
    "    computes the total number of frames in the data\n",
    "    \"\"\"\n",
    "    l = []\n",
    "    for d in data:\n",
    "        l.append(len(d[0]))\n",
    "    return l\n",
    "\n",
    "def get_clean(file_name):\n",
    "    \"\"\"\n",
    "    fetches the cleaned data as well as the total time (in sec) of audio\n",
    "    \"\"\"\n",
    "    with open('data.txt', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    lengths = get_lengths(data, sampling_rate)\n",
    "    return data, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shortest audio is of 4384 frames while the longest of 100402 and\n",
      "the avertage number of frame is 20970.730299667037.\n"
     ]
    }
   ],
   "source": [
    "clean, lengths = get_clean(FILE_NAME)\n",
    "min_ = np.min(lengths)\n",
    "max_ = np.max(lengths)\n",
    "print(r\"\"\"the shortest audio is of {} frames while the longest of {} and\n",
    "the avertage number of frame is {}.\"\"\".format(np.min(lengths), np.max(lengths), np.mean(lengths)))\n",
    "\n",
    "sample_size = min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(sample_size, sampling_rate):\n",
    "    \"\"\"\n",
    "    separate the samples in chunks of equal size and compute their mfcc\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "#     min_ = np.float('inf')\n",
    "#     max_ = -np.float('inf')\n",
    "    for c in tqdm(clean):\n",
    "        signal = c[0]\n",
    "        signal_label = c[1]\n",
    "        signal_length = len(c[0])\n",
    "        n_samples = int(signal_length / sample_size)\n",
    "        for n in range(n_samples):\n",
    "            a = int(n * sample_size)\n",
    "            b = int((n+1) * sample_size)\n",
    "            s = signal[a:b]\n",
    "            mfcc = librosa.feature.mfcc(\n",
    "                s,\n",
    "                sr = sampling_rate\n",
    "            )\n",
    "            X.append(mfcc)\n",
    "            y.append(signal_label)\n",
    "\n",
    "            # inplace normalizaton but it actually perfomed worse\n",
    "#             M = np.max(mfcc)\n",
    "#             m = np.min(mfcc)\n",
    "#             if M > max_:\n",
    "#                 max_ = M\n",
    "#             if m < min_:\n",
    "#                 min_ = m\n",
    "                \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    #X = (X - min_) / (max_ - min_)\n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2703/2703 [00:35<00:00, 75.27it/s]\n"
     ]
    }
   ],
   "source": [
    "X, y = build_features(min_, sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['X'] = X\n",
    "data['y'] = y\n",
    "\n",
    "with open('mfccs.txt', 'wb') as f:\n",
    "    pickle.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
